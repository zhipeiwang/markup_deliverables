
@article{mitchellBayesianVariableSelection1988,
	title = {Bayesian Variable Selection in Linear Regression},
	volume = {83},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290129},
	doi = {10.2307/2290129},
	abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose γ. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
	pages = {1023--1032},
	number = {404},
	journaltitle = {Journal of the American Statistical Association},
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	urldate = {2024-12-21},
	date = {1988},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	file = {JSTOR Full Text PDF:C\:\\Users\\wzp\\Zotero\\storage\\VHS9CZNV\\Mitchell and Beauchamp - 1988 - Bayesian Variable Selection in Linear Regression.pdf:application/pdf},
}

@article{georgeVariableSelectionGibbs1993,
author = {George, Edward and McCulloch, Robert},
year = {1993},
month = {09},
pages = {881-889},
title = {Variable Selection Via Gibbs Sampling},
volume = {88},
journal = {Journal of The American Statistical Association - J AMER STATIST ASSN},
doi = {10.1080/01621459.1993.10476353}
}

@article{piironenProjectiveInferenceHighdimensional2020,
	title = {Projective Inference in High-dimensional Problems: Prediction and Feature Selection},
	volume = {14},
	issn = {1935-7524},
	url = {http://arxiv.org/abs/1810.02406},
	doi = {10.1214/20-EJS1711},
	shorttitle = {Projective Inference in High-dimensional Problems},
	abstract = {This paper discusses predictive inference and feature selection for generalized linear models with scarce but high-dimensional data. We argue that in many cases one can beneﬁt from a decision theoretically justiﬁed two-stage approach: ﬁrst, construct a possibly non-sparse model that predicts well, and then ﬁnd a minimal subset of features that characterize the predictions. The model built in the ﬁrst step is referred to as the reference model and the operation during the latter step as predictive projection. The key characteristic of this approach is that it ﬁnds an excellent tradeoff between sparsity and predictive accuracy, and the gain comes from utilizing all available information including prior and that coming from the left out features. We review several methods that follow this principle and provide novel methodological contributions. We present a new projection technique that uniﬁes two existing techniques and is both accurate and fast to compute. We also propose a way of evaluating the feature selection process using fast leave-one-out cross-validation that allows for easy and intuitive model size selection. Furthermore, we prove a theorem that helps to understand the conditions under which the projective approach could be beneﬁcial. The beneﬁts are illustrated via several simulated and real world examples.},
	number = {1},
	journaltitle = {Electronic Journal of Statistics},
	shortjournal = {Electron. J. Statist.},
	author = {Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki},
	urldate = {2024-09-21},
	date = {2020-01-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.02406 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:C\:\\Users\\wzp\\Zotero\\storage\\IA25K4R2\\Piironen et al. - 2020 - Projective Inference in High-dimensional Problems Prediction and Feature Selection.pdf:application/pdf},
}
